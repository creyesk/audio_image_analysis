{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks on Tensorflow and Keras\n",
    "\n",
    "![title](keras-tensorflow-logo.jpg)\n",
    "![title](vgg16.ppm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a convolutional layer from scratch on TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(X, W, b, padding, stride):\n",
    "    n, h, w, c = map(lambda d: d.value, X.get_shape())\n",
    "    filter_h, filter_w, filter_c, filter_n = [d.value \n",
    "                                              for d in W.get_shape()]\n",
    "    \n",
    "    out_h = (h + 2*padding - filter_h)//stride + 1\n",
    "    out_w = (w + 2*padding - filter_w)//stride + 1\n",
    "\n",
    "    X_flat = flatten(X, filter_h, filter_w, filter_c,\n",
    "                     out_h, out_w, stride, padding)\n",
    "    W_flat = tf.reshape(W, [filter_h*filter_w*filter_c, filter_n])\n",
    "    \n",
    "    z = tf.matmul(X_flat, W_flat) + b     # b: 1 X filter_n\n",
    "    \n",
    "    return tf.transpose(tf.reshape(z, [out_h, out_w, n, filter_n]), [2, 0, 1, 3])\n",
    "\n",
    "def flatten(X, window_h, window_w, window_c,\n",
    "            out_h, out_w, stride=1, padding=0):\n",
    "    \n",
    "    X_padded = tf.pad(X, [[0,0], [padding, padding],\n",
    "                          [padding, padding], [0,0]])\n",
    "\n",
    "    windows = []\n",
    "    for y in range(out_h):\n",
    "        for x in range(out_w):\n",
    "            window = tf.slice(X_padded,\n",
    "                              [0, y*stride, x*stride, 0],\n",
    "                              [-1, window_h, window_w, -1])\n",
    "            windows.append(window)\n",
    "    stacked = tf.stack(windows) \n",
    "    # shape : [out_h, out_w, n, filter_h, filter_w, c]\n",
    "\n",
    "    return tf.reshape(stacked, [-1, window_c*window_w*window_h])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An easier way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=input_layer,\n",
    "    filters=64,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need some data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# input image dimensions\n",
    "num_classes = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The easiest way: Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "num_classes = 10\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.2665 - acc: 0.9176 - val_loss: 0.0562 - val_acc: 0.9813\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0883 - acc: 0.9735 - val_loss: 0.0393 - val_acc: 0.9859\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0646 - acc: 0.9808 - val_loss: 0.0400 - val_acc: 0.9864\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0539 - acc: 0.9838 - val_loss: 0.0327 - val_acc: 0.9892\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0469 - acc: 0.9858 - val_loss: 0.0266 - val_acc: 0.9915\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0397 - acc: 0.9878 - val_loss: 0.0295 - val_acc: 0.9905\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0367 - acc: 0.9891 - val_loss: 0.0311 - val_acc: 0.9890\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0343 - acc: 0.9895 - val_loss: 0.0262 - val_acc: 0.9916\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0314 - acc: 0.9904 - val_loss: 0.0281 - val_acc: 0.9903\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0286 - acc: 0.9910 - val_loss: 0.0276 - val_acc: 0.9909\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0264 - acc: 0.9918 - val_loss: 0.0287 - val_acc: 0.9915\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0252 - acc: 0.9920 - val_loss: 0.0273 - val_acc: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb1e44e32b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 12\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.02729151461883266\n",
      "Test accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model with different kinds of Inputs. Model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, LSTM, Input, Dropout\n",
    "from keras.layers import concatenate, RepeatVector\n",
    "from keras.optimizers import sgd\n",
    "from keras.callbacks import EarlyStopping\n",
    "maxlen = 61\n",
    "\n",
    "OT_input  = Input(shape = (64,))\n",
    "OT_dense1  = Dense(64,  activation = 'relu')(OT_input)\n",
    "OT_dense2  = Dense(64,  activation = 'relu')(OT_dense1)\n",
    "\n",
    "OT_dropout = Dropout(0.5)(OT_dense2)\n",
    "OT_repeat = RepeatVector(maxlen)(OT_dropout)\n",
    "\n",
    "SEG_input = Input(shape=(61,2))\n",
    "SEG_lstm   = LSTM(128, return_sequences = True, activation=\"relu\")(SEG_input)\n",
    "\n",
    "merge = concatenate([OT_repeat, SEG_lstm])\n",
    "merge_dense = Dense(128, activation = 'relu')(merge)\n",
    "\n",
    "dropout_merge = Dropout(0.5)(merge_dense)\n",
    "final = Dense(1, activation = 'sigmoid')(dropout_merge)\n",
    "\n",
    "model = Model(inputs=[OT_input, SEG_input], outputs=final)\n",
    "\n",
    "opt = sgd(lr=0.01, decay=0.0001, momentum=0.5, nesterov=True)\n",
    "callback = EarlyStopping(monitor='loss',min_delta=0.0001, patience=1, verbose=0, mode='auto')\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', sample_weight_mode = 'temporal',\n",
    "               metrics=['accuracy'])\n",
    "num_epochs = 20\n",
    "# batch_size = 1024\n",
    "\n",
    "# model.fit([X_train_OT,X_train_SEG], y_train, batch_size=batch_size, sample_weight= X_train_SEG[...,1],\n",
    "#           epochs=num_epochs, verbose=1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Keras Library\n",
    "### Layers:\n",
    "- Dense\n",
    "- Flatten\n",
    "- Dropout\n",
    "- Masking\n",
    "- Convolutional\n",
    "- Pooling\n",
    "- Recurrent\n",
    "- Merge\n",
    "- etc\n",
    "\n",
    "### Activations:\n",
    "- relu\n",
    "- softmax\n",
    "- tanh\n",
    "- sigmoid\n",
    "- etc\n",
    "\n",
    "\n",
    "### Preprocessing:\n",
    "- Sequence\n",
    "- Text\n",
    "- Images\n",
    "\n",
    "### Losses\n",
    "- MSE + MAE + MAPE\n",
    "- Hinge + Categorical Hinge\n",
    "- Categorical crossentropy\n",
    "- Kullback-Leibler divergence\n",
    "- Cosine proximity\n",
    "\n",
    "### Metrics:\n",
    "- Accuracy\n",
    "- Top k Accuracy\n",
    "- Custom_metric(y_true, y_pred)\n",
    "\n",
    "### Optimizers\n",
    "- SGD\n",
    "- RMSprop\n",
    "- Adagrad\n",
    "- Adadelta\n",
    "- Adam\n",
    "- etc\n",
    "\n",
    "### Regularizers\n",
    "- l1 + l2\n",
    "- Custom\n",
    "\n",
    "### Callbacks\n",
    "- EarlyStoppin\n",
    "- Logging\n",
    "- TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to install tensorflow-gpu\n",
    "\n",
    "- CUDA Toolkit(NVIDIA) 9.0\n",
    "- CUDNN\n",
    "- pip install tensorflow-gpu\n",
    "- pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "def make_model(conv1_size, d2=0.5):\n",
    "    input_1 = Input(shape=(28,28,1))\n",
    "    conv_1 = Conv2D(conv1_size, (3, 3), activation='relu')(input_1)\n",
    "    conv_2 = Conv2D(64, (3, 3), activation='relu')(conv_1)\n",
    "    pool_1 = MaxPooling2D(pool_size=(2, 2))(conv_2)\n",
    "    drop_1 = Dropout(0.25)(pool_1)\n",
    "    flat_1 = Flatten()(drop_1)\n",
    "    dense_1 = Dense(128, activation='relu')(flat_1)\n",
    "    drop_2 = Dropout(d2)(dense_1)\n",
    "    final = Dense(num_classes, activation='softmax')(drop_2)\n",
    "    \n",
    "    model = Model(inputs=input_1, outputs=final)\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch integration with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_model() missing 1 required positional argument: 'conv1_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2ee08863eac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                            param_grid=param_grid, refit=False, cv=2)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 **self.filter_sk_params(self.build_fn.__call__))\n\u001b[1;32m    139\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_sk_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mloss_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: make_model() missing 1 required positional argument: 'conv1_size'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(build_fn=make_model,\n",
    "                        batch_size=32, epochs=12)\n",
    "param_grid = {'batch_size': [32, 128, 512], 'd2': [0.5]}\n",
    "grid_search = GridSearchCV(estimator=model,\n",
    "                           param_grid=param_grid, refit=False, cv=2)\n",
    "\n",
    "grid_result = grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... or do it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.2823 - acc: 0.9133 - val_loss: 0.0592 - val_acc: 0.9799\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0909 - acc: 0.9733 - val_loss: 0.0450 - val_acc: 0.9847\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0665 - acc: 0.9802 - val_loss: 0.0358 - val_acc: 0.9882\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0540 - acc: 0.9845 - val_loss: 0.0312 - val_acc: 0.9894\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.2603 - acc: 0.9192 - val_loss: 0.0545 - val_acc: 0.9827\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0854 - acc: 0.9752 - val_loss: 0.0398 - val_acc: 0.9866\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0638 - acc: 0.9815 - val_loss: 0.0399 - val_acc: 0.9859\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0539 - acc: 0.9841 - val_loss: 0.0329 - val_acc: 0.9898\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.5297 - acc: 0.8359 - val_loss: 0.1191 - val_acc: 0.9622\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1465 - acc: 0.9571 - val_loss: 0.0737 - val_acc: 0.9762\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1044 - acc: 0.9682 - val_loss: 0.0578 - val_acc: 0.9811\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0836 - acc: 0.9744 - val_loss: 0.0483 - val_acc: 0.9845\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.5163 - acc: 0.8400 - val_loss: 0.1093 - val_acc: 0.9659\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.1406 - acc: 0.9596 - val_loss: 0.0643 - val_acc: 0.9791\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.0991 - acc: 0.9703 - val_loss: 0.0668 - val_acc: 0.9779\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.0764 - acc: 0.9774 - val_loss: 0.0429 - val_acc: 0.9858\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def dict_product(d):\n",
    "    return (dict(zip(d, x)) for x in itertools.product(*d.values()))\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras.backend as K\n",
    "\n",
    "param_grid = {'batch_size': [128, 512],\n",
    "              'conv1_size': [32, 128]}\n",
    "for d in dict_product(param_grid):\n",
    "    K.clear_session()\n",
    "    tbCallBack = TensorBoard(log_dir='./Graph/'+\\\n",
    "                             '_'.join(['{}({})'.format(k,v)\n",
    "                                       for k, v in d.items()]),\n",
    "                             histogram_freq=1,\n",
    "                             write_graph=True,\n",
    "                             write_images=False)\n",
    "    model = make_model(conv1_size=d['conv1_size'])\n",
    "    model.fit(x_train, y_train,\n",
    "          batch_size=d['batch_size'],\n",
    "          epochs=4,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
